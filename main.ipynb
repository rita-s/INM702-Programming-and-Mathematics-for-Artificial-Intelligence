{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INM702 Programming and Mathematics for Artificial Intelligence Coursework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Loading and Preprocessing the MNIST Dataset**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Loading MNIST the dataset and preprocessing it (normalize pixel values, reshape, etc.)\n",
    "\n",
    "# Defining function\n",
    "def load_mnist_and_print():\n",
    "    \n",
    "    # Loading the MNIST dataset\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Splitting the dataset into training and testing sets, normalising and reshaping the data\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "\n",
    "\n",
    "    # Printing a small part of the dataset\n",
    "    print(\"X_train sample shape:\", X_train.shape)\n",
    "    print(\"X_train sample:\", X_train[0])\n",
    "    print(\"y_train sample:\", y_train[0])\n",
    "    print(\"X_test sample shape:\", X_test.shape)\n",
    "    print(\"X_test sample:\", X_test[0])\n",
    "    print(\"y_test sample:\", y_test[0])\n",
    "\n",
    "# Calling the function\n",
    "load_mnist_and_print()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The output indicates:\n",
    "\n",
    "# X_train sample shape: This shows that we have 60,000 samples in the training set, each flattened into a vector of 784 elements \n",
    "#(as the original 28x28 pixel images are flattened).\n",
    "\n",
    "# X_train sample: This is the first image in the training set, represented as an 1D array of normalised pixel values (ranging from 0 to 1).\n",
    "\n",
    "# y_train sample: This indicates the label of the first image in the training dataset, which is \"5\" in this case.\n",
    "\n",
    "# X_test sample shape: This shows the shape of the test dataset with 10,000 samples, also flattened into vectors of 784 elements.\n",
    "\n",
    "# X_test sample: This represents the first image in the test set, similar to the training set.\n",
    "\n",
    "# y_test sample: The label for the first image in the test set, which is \"7\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining a function to plot a single image from the dataset:\n",
    "\n",
    "def plot_image(data, index):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    # Using the original X_train for visualisation\n",
    "    plt.imshow(data[index], cmap='gray')\n",
    "    plt.title(f\"Label: {y_train[index]}\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualising the image at a specific index\n",
    "plot_image(X_train, 0)  # This is index 0, we can see other images by changing the index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Defining a function to plot multiple images from the dataset:\n",
    "\n",
    "def plot_images(data, labels, start_index, num_images):\n",
    "    plt.figure(figsize=(8,8)) # Creating a figure with a specified size (8x8 in this case)\n",
    "    # Looping through the specified number of images\n",
    "    for i in range(num_images):\n",
    "        # Creating a subplot for each image\n",
    "        plt.subplot(5, 5, i+1)  # Defining a 5x5 grid size and placing each image in sequence\n",
    "        plt.xticks([]) # Removing the x-axis tick marks\n",
    "        plt.yticks([]) # Removing the y-axis tick marks\n",
    "        # plt.grid(False) # This line doesn't have a functional impact on the output as images are being displayed without a grid,\n",
    "        # but usually it is used to toggle the visibility of the grid within a plot\n",
    "        plt.imshow(data[start_index + i], cmap=plt.cm.binary)  # Displaying the image at the given index in grayscale\n",
    "        plt.xlabel(f\"Label: {labels[start_index + i]}\") # Adding a label below the image showing its corresponding label from the dataset\n",
    "    plt.show() # Displaying the entire set of subplots\n",
    "\n",
    "# Plotting the first 25 images from the dataset\n",
    "plot_images(X_train, y_train, 0, 25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Neural Network Class - Implement a neural network class with:\n",
    "# Sigmoid and ReLU activation functions\n",
    "# Softmax layer for classification\n",
    "# Dropout for regularization\n",
    "# Configurable architecture and parameters\n",
    "\n",
    "# Forward and Backward Pass:\n",
    "# Implement the forward pass to compute predictions\n",
    "# Implement the backward pass to compute gradients using backpropagation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Implementing sigmoid and ReLU layers**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For this sub-task, you should implement forward and backward pass for sigmoid and ReLU\n",
    "# You should consider presenting these activation functions in the report with any pros cons if they have"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Implementing softmax layer**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Implement softmax with both forward and backward pass\n",
    "# Present the softmax in the report along with any numerical issues when calculating the softmax function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Implementing dropout**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Present dropout in the report\n",
    "# Implement inverted dropout\n",
    "# Forward and backward pass should be implemented\n",
    "# Note: Since the test performance is critical, it is also preferable to leaving the forward pass unchanged at test time\n",
    "# Therefore, in most implementations inverted dropout is employed to overcome the undesirable property of the original dropout\n",
    "\n",
    "# Loss Function:\n",
    "# Use a suitable loss function (e.g., cross-entropy) for classification tasks"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Implementing a fully parametrizable neural network class**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Implement a fully-connected NN class where with number of hidden layers, units, activation functions can be changed \n",
    "# In addition, add dropout or regularizer (L1 or L2)\n",
    "# Report the parameters used (update rule, learning rate, decay, epochs, batch size) and include the plotsin your report"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Implementing optimizer**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Implement any two optimizers of your choice\n",
    "# Briefly present the optimizers in the report\n",
    "# The optimizers can be flavours of gradient descent. For instance: Stochastic gradient descent (SGD) and SGD with momentum\n",
    "# SGD and mini-batch gradient descent, etc\n",
    "\n",
    "\n",
    "# Optimizer:\n",
    "# Implement an optimizer (e.g., SGD, Adam) to update weights based on gradients\n",
    "\n",
    "# Training Loop:\n",
    "# Train your neural network using the training set\n",
    "# Monitor performance on the validation set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Evaluating different neural network architectures/parameters, presenting and discussing the results**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Be creative in the analysis and discussion.\n",
    "# Evaluate different hyperparameters.\n",
    "# For instance: different network architectures, activation functions, comparison of optimizers, L1/L2 performance comparison with dropout, etc. \n",
    "# Support your results with plots/graph and discussion\n",
    "\n",
    "\n",
    "# Evaluation:\n",
    "# Evaluate different architectures and parameters:\n",
    "# Vary the number of layers, neurons per layer, learning rate, dropout rates, etc\n",
    "# Use a validation set to choose the best-performing model\n",
    "\n",
    "# Testing:\n",
    "# Assess the final model on the test set to estimate its generalization performance\n",
    "\n",
    "# Analysis:\n",
    "# Analyze the results, understand the impact of different parameters on performance, and draw conclusions\n",
    "\n",
    "# Documentation:\n",
    "# Keep track of configurations, results, and any observations during the experimentation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement any two optimizers of your choice\n",
    "# Briefly present the optimizers in the report\n",
    "# The optimizers can be flavours of gradient descent. For instance: Stochastic gradient descent (SGD) and SGD with momentum\n",
    "# SGD and mini-batch gradient descent, etc\n",
    "\n",
    "\n",
    "# Optimizer:\n",
    "# Implement an optimizer (e.g., SGD, Adam) to update weights based on gradients\n",
    "\n",
    "# Training Loop:\n",
    "# Train your neural network using the training set\n",
    "# Monitor performance on the validation set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating different neural network architectures/parameters, presenting and discussing the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be creative in the analysis and discussion.\n",
    "# Evaluate different hyperparameters.\n",
    "# For instance: different network architectures, activation functions, comparison of optimizers, L1/L2 performance comparison with dropout, etc. \n",
    "# Support your results with plots/graph and discussion\n",
    "\n",
    "\n",
    "# Evaluation:\n",
    "# Evaluate different architectures and parameters:\n",
    "# Vary the number of layers, neurons per layer, learning rate, dropout rates, etc\n",
    "# Use a validation set to choose the best-performing model\n",
    "\n",
    "# Testing:\n",
    "# Assess the final model on the test set to estimate its generalization performance\n",
    "\n",
    "# Analysis:\n",
    "# Analyze the results, understand the impact of different parameters on performance, and draw conclusions\n",
    "\n",
    "# Documentation:\n",
    "# Keep track of configurations, results, and any observations during the experimentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}