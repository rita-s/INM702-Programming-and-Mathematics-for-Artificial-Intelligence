{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INM702 Programming and Mathematics for Artificial Intelligence Coursework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Loading and Preprocessing the MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train sample shape: (60000, 784)\n",
      "X_train sample: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "y_train sample: 5\n",
      "X_test sample shape: (10000, 784)\n",
      "X_test sample: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.32941176 0.7254902\n",
      " 0.62352941 0.59215686 0.23529412 0.14117647 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.87058824 0.99607843 0.99607843 0.99607843\n",
      " 0.99607843 0.94509804 0.77647059 0.77647059 0.77647059 0.77647059\n",
      " 0.77647059 0.77647059 0.77647059 0.77647059 0.66666667 0.20392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.2627451  0.44705882 0.28235294 0.44705882 0.63921569 0.89019608\n",
      " 0.99607843 0.88235294 0.99607843 0.99607843 0.99607843 0.98039216\n",
      " 0.89803922 0.99607843 0.99607843 0.54901961 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.06666667 0.25882353 0.05490196\n",
      " 0.2627451  0.2627451  0.2627451  0.23137255 0.08235294 0.9254902\n",
      " 0.99607843 0.41568627 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3254902  0.99215686 0.81960784 0.07058824\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.08627451\n",
      " 0.91372549 1.         0.3254902  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.50588235 0.99607843 0.93333333\n",
      " 0.17254902 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.23137255 0.97647059 0.99607843 0.24313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.52156863 0.99607843\n",
      " 0.73333333 0.01960784 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.03529412 0.80392157 0.97254902 0.22745098 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.49411765\n",
      " 0.99607843 0.71372549 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.29411765 0.98431373 0.94117647 0.22352941\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.0745098\n",
      " 0.86666667 0.99607843 0.65098039 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.79607843 0.99607843 0.85882353\n",
      " 0.1372549  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.14901961 0.99607843 0.99607843 0.30196078 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.12156863 0.87843137 0.99607843\n",
      " 0.45098039 0.00392157 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.52156863 0.99607843 0.99607843 0.20392157 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.23921569 0.94901961\n",
      " 0.99607843 0.99607843 0.20392157 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.4745098  0.99607843 0.99607843 0.85882353\n",
      " 0.15686275 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.4745098  0.99607843 0.81176471 0.07058824 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "y_test sample: 7\n"
     ]
    }
   ],
   "source": [
    "# Loading MNIST the dataset and preprocessing it (normalize pixel values, reshape, etc.)\n",
    "\n",
    "# Defining function\n",
    "def load_mnist_and_print():\n",
    "    \n",
    "    # Loading the MNIST dataset\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Splitting the dataset into training and testing sets, normalising and reshaping the data\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "\n",
    "\n",
    "    # Printing a small part of the dataset\n",
    "    print(\"X_train sample shape:\", X_train.shape)\n",
    "    print(\"X_train sample:\", X_train[0])\n",
    "    print(\"y_train sample:\", y_train[0])\n",
    "    print(\"X_test sample shape:\", X_test.shape)\n",
    "    print(\"X_test sample:\", X_test[0])\n",
    "    print(\"y_test sample:\", y_test[0])\n",
    "\n",
    "# Calling the function\n",
    "load_mnist_and_print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The output indicates:\n",
    "\n",
    "# X_train sample shape: This shows that we have 60,000 samples in the training set, each flattened into a vector of 784 elements \n",
    "#(as the original 28x28 pixel images are flattened).\n",
    "\n",
    "# X_train sample: This is the first image in the training set, represented as an 1D array of normalised pixel values (ranging from 0 to 1).\n",
    "\n",
    "# y_train sample: This indicates the label of the first image in the training dataset, which is \"5\" in this case.\n",
    "\n",
    "# X_test sample shape: This shows the shape of the test dataset with 10,000 samples, also flattened into vectors of 784 elements.\n",
    "\n",
    "# X_test sample: This represents the first image in the test set, similar to the training set.\n",
    "\n",
    "# y_test sample: The label for the first image in the test set, which is \"7\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Defining a function to plot a single image from the dataset:\n",
    "\n",
    "def plot_image(data, index):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    # Using the original X_train for visualisation\n",
    "    plt.imshow(data[index], cmap='gray')\n",
    "    plt.title(f\"Label: {y_train[index]}\")\n",
    "    plot_image(X_train, 0)\n",
    "    plt.show()\n",
    "\n",
    "# Visualising the image at a specific index\n",
    "#plot_image(X_train, 0)  # This is index 0, we can see other images by changing the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Defining a function to plot multiple images from the dataset:\n",
    "\n",
    "def plot_images(data, labels, start_index, num_images):\n",
    "    plt.figure(figsize=(8,8)) # Creating a figure with a specified size (8x8 in this case)\n",
    "    # Looping through the specified number of images\n",
    "    for i in range(num_images):\n",
    "        # Creating a subplot for each image\n",
    "        plt.subplot(5, 5, i+1)  # Defining a 5x5 grid size and placing each image in sequence\n",
    "        plt.xticks([]) # Removing the x-axis tick marks\n",
    "        plt.yticks([]) # Removing the y-axis tick marks\n",
    "        # plt.grid(False) # This line doesn't have a functional impact on the output as images are being displayed without a grid,\n",
    "        # but usually it is used to toggle the visibility of the grid within a plot\n",
    "        plt.imshow(data[start_index + i], cmap=plt.cm.binary)  # Displaying the image at the given index in grayscale\n",
    "        plt.xlabel(f\"Label: {labels[start_index + i]}\") # Adding a label below the image showing its corresponding label from the dataset\n",
    "        plot_images(X_train, y_train, 0, 25)\n",
    "        plt.show() # Displaying the entire set of subplots\n",
    "\n",
    "# Plotting the first 25 images from the dataset\n",
    "#plot_images(X_train, y_train, 0, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Neural Network Class - Implement a neural network class with:\n",
    "# Sigmoid and ReLU activation functions\n",
    "# Softmax layer for classification\n",
    "# Dropout for regularization\n",
    "# Configurable architecture and parameters\n",
    "\n",
    "# Forward and Backward Pass:\n",
    "# Implement the forward pass to compute predictions\n",
    "# Implement the backward pass to compute gradients using backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Implementing sigmoid and ReLU layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call (3878982295.py, line 78)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[63], line 78\u001b[1;36m\u001b[0m\n\u001b[1;33m    for key(k), delta in w_delta.items():\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot assign to function call\n"
     ]
    }
   ],
   "source": [
    "# For this sub-task, you should implement forward and backward pass for sigmoid and ReLU\n",
    "# You should consider presenting these activation functions in the report with any pros cons if they have\n",
    "class Network():\n",
    "    def init_(self, sizes=[784, 128, 64,10], epochs=1000, lr=0.01):\n",
    "        self.sizes = sizes\n",
    "        self.epochs=epochs\n",
    "        self.lr= lr\n",
    "        input_layer=sizes[0]\n",
    "        hidden_1=sizes[1]\n",
    "        hidden_2=sizes[2] \n",
    "        output_layers =sizes[3] #input layers nd output layers dont have weights\n",
    "                                #only hidden layers have weights\n",
    "        self.params ={ \n",
    "         'W1': np.random.randn(hidden_1, input_layer)* np.sqrt(1./hidden_1), # weight matrix input layer\n",
    "         'b1': np.random.randn(hidden_1,1)*np.sqrt(1./10),\n",
    "         'w2': np.random.randn(hidden_2, hidden_1)* np.sqrt(1./hidden_2),# weight matrix first hidden and second hidden layer\n",
    "         'b2': np.random.randn(hidden_2,1)*np.sqrt(1./128),\n",
    "         'w3': np.random.randn(output_layer, hidden_2)* np.sqrt(1./output_layer), # weight matrix second hidden layer and output layer\n",
    "         'b3': np.random.randn(output_layer,1)*np.sqrt(1./input_layer)}#784\n",
    "        #For sigmoid\n",
    "        #def a sigmoid function\n",
    "    def sigmoid(self, X, derivative=False):\n",
    "        if derivative:\n",
    "            return (np.exp(-X))/((np.exp(-X)+1)**2)\n",
    "        return 1/(1+np.exp(-X))\n",
    "    \n",
    "    def softmax(self, X, derivative=False):\n",
    "        exps = np.exp(X-X.max())\n",
    "        if derivative:\n",
    "            return exps /np.sum(exps, axis=0)* (1-exps/ np.sum(exps, axis=0))\n",
    "        return exps/np.sum(exps, axis=0)\n",
    "        \n",
    "        \n",
    "    def forward_pass(self, X_train):\n",
    "        params = self.params     #it will be use in backward pass\n",
    "    \n",
    "    #def the params\n",
    "    params['A0'] = X_train      \n",
    "    # activations of layers 1, W1 is weight matrix of input layer\n",
    "    params['Z1'] =np.dot(params['W1'], params ['A0'])+ params['b1']\n",
    "    #Sigmoid here\n",
    "    params['A1'] =self.sigmoid(params['Z1'])\n",
    "    \n",
    "    # activations of layers 2, W2 is weight matrix of hidden_1 to hidden_2 layer  and 'A1' is input to this layer\n",
    "    params['Z2'] =np.dot(params['W2'], params ['A1']) + params['b2']\n",
    "    #Sigmoid here\n",
    "    params['A2'] =self.sigmoid(params['Z2'])\n",
    "    \n",
    "    # activations of layers 3, W3 is weight matrix of hidden_2 to output layer  and 'A2' is input to this layer\n",
    "    params['Z3'] =np.dot(params['W3'], params ['A2']) + params['b3']\n",
    "    #Softmax here\n",
    "    params['A3'] =self.softmax(params['Z3'])\n",
    "    \n",
    "\n",
    "    #For Sigmoid\n",
    "    \n",
    "    def backward_pass(self, y_train, output):\n",
    "        params= self.params\n",
    "        \n",
    "        w_delta={}\n",
    "        # calculate W3 update\n",
    "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], derivative=True)\n",
    "        w3_delta['W3'] = np.outer(error, params['A2'])\n",
    "\n",
    "      # Calculate W2 update\n",
    "        error = np.dot(params['W3'].T, error) * self.sigmoid(params['Z2'], derivative=True)\n",
    "        w2_delta['W2'] = np.outer(error, params['A1'])\n",
    "\n",
    "      # Calculate W1 update\n",
    "        error = np.dot(params['W2'].T, error) * self.sigmoid(params['Z1'], derivative=True)\n",
    "        w1_delta['W1'] = np.outer(error, params['A0'])\n",
    "\n",
    "       \n",
    "    def update_network_parameters(self, w_delta):\n",
    "        for key(k), delta in w_delta.items():\n",
    "        self.params[k]-=self.lr*delta\n",
    "        self.params[b]-=self.lr*np.mean(output_error) \n",
    "         return input_error\n",
    "        print (\"shape of Z1\".format(Z1.shape))\n",
    "        print (\"shape of Z2\".format(Z2.shape))\n",
    "        print (\"shape of Z3\".format(Z3.shape))\n",
    "        print (\"shape of w1_delta\".format(w1_delta.shape))\n",
    "        print (\"shape of w2_delta\".format(w2_delta.shape))\n",
    "        print (\"shape of w3_delta\".format(w3_delta.shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR ReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def init_(self, sizes=[784, 128, 64,10], epochs=1000, lr=0.01):\n",
    "        self.sizes = sizes\n",
    "        self.epochs=epochs\n",
    "        self.lr= lr\n",
    "        input_layer=sizes[0]\n",
    "        hidden_1=sizes[1]\n",
    "        hidden_2=sizes[2] \n",
    "        output_layers =sizes[3] #input layers nd output layers dont have weights\n",
    "                                #only hidden layers have weights\n",
    "        self.params ={ \n",
    "         'W1': np.random.randn(hidden_1, input_layer)* np.sqrt(1./hidden_1), # weight matrix input layer\n",
    "         'b1': np.random.randn(hidden_1,1)*np.sqrt(1./10),\n",
    "         'w2': np.random.randn(hidden_2, hidden_1)* np.sqrt(1./hidden_2),# weight matrix first hidden and second hidden layer\n",
    "         'b2': np.random.randn(hidden_2,1)*np.sqrt(1./128),\n",
    "         'w3': np.random.randn(output_layer, hidden_2)* np.sqrt(1./output_layer), # weight matrix second hidden layer and output layer\n",
    "         'b3': np.random.randn(output_layer,1)*np.sqrt(1./input_layer)}\n",
    "       \n",
    "    def ReLU(self, X, derivative=False):\n",
    "        if derivative:\n",
    "            return(X > 0)\n",
    "        return (np.maximum(X,0))\n",
    "    def softmax(self, X, derivative=False):\n",
    "        exps = np.exp(X-X.max())\n",
    "        if derivative:\n",
    "            return exps /np.sum(exps, axis=0)* (1-exps/ np.sum(exps, axis=0))\n",
    "        return exps/np.sum(exps, axis=0)\n",
    "        \n",
    "        \n",
    "    def forward_pass(self, X_train):\n",
    "        params = self.params     #it will be use in backward pass\n",
    "    \n",
    "    #def the params\n",
    "    params['A0'] = X_train      \n",
    "    # activations of layers 1, W1 is weight matrix of input layer\n",
    "    params['Z1'] =np.dot(params['W1'], params ['A0'])+ params['b1']\n",
    "    #Sigmoid here\n",
    "    params['A1'] =self.ReLU(params['Z1'])\n",
    "    \n",
    "    # activations of layers 2, W2 is weight matrix of hidden_1 to hidden_2 layer  and 'A1' is input to this layer\n",
    "    params['Z2'] =np.dot(params['W2'], params ['A1']) + params['b2']\n",
    "    #Sigmoid here\n",
    "    params['A2'] =self.ReLU(params['Z2'])\n",
    "    \n",
    "    # activations of layers 3, W3 is weight matrix of hidden_2 to output layer  and 'A2' is input to this layer\n",
    "    params['Z3'] =np.dot(params['W3'], params ['A2']) + params['b3']\n",
    "    #Softmax here\n",
    "    params['A3'] =self.softmax(params['Z3'])\n",
    "    \n",
    "\n",
    "    #ForReLU\n",
    "    \n",
    "    def backward_pass(self, y_train, output):\n",
    "        params= self.params\n",
    "        \n",
    "        w_delta={}\n",
    "        # calculate W3 update\n",
    "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(params['Z3'], derivative=True)\n",
    "        w3_delta['W3'] = np.outer(error, params['A2'])\n",
    "\n",
    "      # Calculate W2 update\n",
    "        error = np.dot(params['W3'].T, error) * self.ReLU(params['Z2'], derivative=True)\n",
    "        w2_delta['W2'] = np.outer(error, params['A1'])\n",
    "\n",
    "      # Calculate W1 update\n",
    "        error = np.dot(params['W2'].T, error) * self.ReLU(params['Z1'], derivative=True)\n",
    "        w1_delta['W1'] = np.outer(error, params['A0'])\n",
    "\n",
    "       \n",
    "    def update_network_parameters(self, w_delta):\n",
    "        for key(k), delta in w_delta.items():\n",
    "        self.params[k]-=self.lr*delta\n",
    "        self.params[b]-=self.lr*np.mean(output_error) \n",
    "         return input_error\n",
    "        print (\"shape of Z1\".format(Z1.shape))\n",
    "        print (\"shape of Z2\".format(Z2.shape))\n",
    "        print (\"shape of Z3\".format(Z3.shape))\n",
    "        print (\"shape of w1_delta\".format(w1_delta.shape))\n",
    "        print (\"shape of w2_delta\".format(w2_delta.shape))\n",
    "        print (\"shape of w3_delta\".format(w3_delta.shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Implementing softmax layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Implement softmax with both forward and backward pass\n",
    "# Present the softmax in the report along with any numerical issues when calculating the softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Implementing dropout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Present dropout in the report\n",
    "# Implement inverted dropout\n",
    "# Forward and backward pass should be implemented\n",
    "# Note: Since the test performance is critical, it is also preferable to leaving the forward pass unchanged at test time\n",
    "# Therefore, in most implementations inverted dropout is employed to overcome the undesirable property of the original dropout\n",
    "\n",
    "# Loss Function:\n",
    "# Use a suitable loss function (e.g., cross-entropy) for classification tasks\n",
    "   class Network():\n",
    "    def init_(self, sizes=[784, 128, 64,10], epochs=1000, lr=0.01,dropout_prob=0.5):\n",
    "        self.sizes = sizes\n",
    "        self.epochs=epochs\n",
    "        self.lr= lr\n",
    "        self.dropout_prob=dropout_prob\n",
    "        input_layer=sizes[0]\n",
    "        hidden_1=sizes[1]\n",
    "        hidden_2=sizes[2] \n",
    "        output_layers =sizes[3] \n",
    "        #Initializing the weights\n",
    "        self.params ={ \n",
    "         'W1': np.random.randn(hidden_1, input_layer)* np.sqrt(1./hidden_1), # weight matrix input layer\n",
    "         'b1': np.random.randn(hidden_1,1)*np.sqrt(1./10),\n",
    "         'w2': np.random.randn(hidden_2, hidden_1)* np.sqrt(1./hidden_2),# weight matrix first hidden and second hidden layer\n",
    "         'b2': np.random.randn(hidden_2,1)*np.sqrt(1./128),\n",
    "         'w3': np.random.randn(output_layer, hidden_2)* np.sqrt(1./output_layer), # weight matrix second hidden layer and output layer\n",
    "         'b3': np.random.randn(output_layer,1)*np.sqrt(1./input_layer)}\n",
    "       # intializing masks\n",
    "         self.dropout_mask={}\n",
    "    \n",
    "    #for sigmoid\n",
    "     #For sigmoid\n",
    "        #def a sigmoid function\n",
    "    def sigmoid(self, X, derivative=False):\n",
    "        if derivative:\n",
    "            return (np.exp(-X))/((np.exp(-X)+1)**2)\n",
    "        return 1/(1+np.exp(-X))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Implementing a fully parametrizable neural network class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Implement a fully-connected NN class where with number of hidden layers, units, activation functions can be changed \n",
    "# In addition, add dropout or regularizer (L1 or L2)\n",
    "# Report the parameters used (update rule, learning rate, decay, epochs, batch size) and include the plotsin your report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Implementing optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Implement any two optimizers of your choice\n",
    "# Briefly present the optimizers in the report\n",
    "# The optimizers can be flavours of gradient descent. For instance: Stochastic gradient descent (SGD) and SGD with momentum\n",
    "# SGD and mini-batch gradient descent, etc\n",
    "\n",
    "\n",
    "# Optimizer:\n",
    "# Implement an optimizer (e.g., SGD, Adam) to update weights based on gradients\n",
    "\n",
    "# Training Loop:\n",
    "# Train your neural network using the training set\n",
    "# Monitor performance on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Evaluating different neural network architectures/parameters, presenting and discussing the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Be creative in the analysis and discussion.\n",
    "# Evaluate different hyperparameters.\n",
    "# For instance: different network architectures, activation functions, comparison of optimizers, L1/L2 performance comparison with dropout, etc. \n",
    "# Support your results with plots/graph and discussion\n",
    "\n",
    "\n",
    "# Evaluation:\n",
    "# Evaluate different architectures and parameters:\n",
    "# Vary the number of layers, neurons per layer, learning rate, dropout rates, etc\n",
    "# Use a validation set to choose the best-performing model\n",
    "\n",
    "# Testing:\n",
    "# Assess the final model on the test set to estimate its generalization performance\n",
    "\n",
    "# Analysis:\n",
    "# Analyze the results, understand the impact of different parameters on performance, and draw conclusions\n",
    "\n",
    "# Documentation:\n",
    "# Keep track of configurations, results, and any observations during the experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement any two optimizers of your choice\n",
    "# Briefly present the optimizers in the report\n",
    "# The optimizers can be flavours of gradient descent. For instance: Stochastic gradient descent (SGD) and SGD with momentum\n",
    "# SGD and mini-batch gradient descent, etc\n",
    "\n",
    "\n",
    "# Optimizer:\n",
    "# Implement an optimizer (e.g., SGD, Adam) to update weights based on gradients\n",
    "\n",
    "# Training Loop:\n",
    "# Train your neural network using the training set\n",
    "# Monitor performance on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating different neural network architectures/parameters, presenting and discussing the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be creative in the analysis and discussion.\n",
    "# Evaluate different hyperparameters.\n",
    "# For instance: different network architectures, activation functions, comparison of optimizers, L1/L2 performance comparison with dropout, etc. \n",
    "# Support your results with plots/graph and discussion\n",
    "\n",
    "\n",
    "# Evaluation:\n",
    "# Evaluate different architectures and parameters:\n",
    "# Vary the number of layers, neurons per layer, learning rate, dropout rates, etc\n",
    "# Use a validation set to choose the best-performing model\n",
    "\n",
    "# Testing:\n",
    "# Assess the final model on the test set to estimate its generalization performance\n",
    "\n",
    "# Analysis:\n",
    "# Analyze the results, understand the impact of different parameters on performance, and draw conclusions\n",
    "\n",
    "# Documentation:\n",
    "# Keep track of configurations, results, and any observations during the experimentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
