{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef19c3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The second task is about implementing deep learning networks using PyTorch. You can use\n",
    "#any imaging or time-series dataset of your choice for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb713543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d83ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(\"Device:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc1e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test data directory\n",
    "data_dir = \"scenes-db/seg_train/seg_train/\"\n",
    "test_data_dir = \"scenes-db/seg_test/seg_test\"\n",
    "\n",
    "\n",
    "#load the train and test data\n",
    "dataset = ImageFolder(data_dir,transform = transforms.Compose([\n",
    "    transforms.Resize((150,150)),transforms.ToTensor()\n",
    "]))\n",
    "test_dataset = ImageFolder(test_data_dir,transforms.Compose([\n",
    "    transforms.Resize((150,150)),transforms.ToTensor()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd11d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = dataset[0]\n",
    "print(img.shape,label)\n",
    "print(\"Follwing classes are there : \\n\",dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e0b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img(img,label):\n",
    "    print(f\"Label : {dataset.classes[label]}\")\n",
    "    plt.imshow(img.permute(1,2,0))\n",
    "\n",
    "#display the first image in the dataset\n",
    "display_img(*dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Define Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f39c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 128\n",
    "val_size = 2000\n",
    "train_size = len(dataset) - val_size \n",
    "\n",
    "train_data,val_data = random_split(dataset,[train_size,val_size])\n",
    "print(f\"Length of Train Data : {len(train_data)}\")\n",
    "print(f\"Length of Validation Data : {len(val_data)}\")\n",
    "\n",
    "#output\n",
    "#Length of Train Data : 12034\n",
    "#Length of Validation Data : 2000\n",
    "\n",
    "#load the train and validation into batches.\n",
    "train_dl = DataLoader(train_data, batch_size, shuffle = True, num_workers = 4, pin_memory = True)\n",
    "val_dl = DataLoader(val_data, batch_size*2, num_workers = 4, pin_memory = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d7044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_batch(dl):\n",
    "    \"\"\"Plot images grid of single batch\"\"\"\n",
    "    for images, labels in dl:\n",
    "        fig,ax = plt.subplots(figsize = (16,12))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images,nrow=16).permute(1,2,0))\n",
    "        break\n",
    "        \n",
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d425de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607231c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, 32, kernel_size = 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32,64, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        \n",
    "            nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128 ,128, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(128, 256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,256, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(82944,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,6)\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb):\n",
    "        return self.network(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a366b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time \n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "  \n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    for batch in val_loader:\n",
    "        images, labels = batch \n",
    "        images,labels = images.to(device), labels.to(device)\n",
    "        out = model(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        outputs.append({'val_loss': loss.detach(), 'val_acc': acc})\n",
    "    \n",
    "    \n",
    "    batch_losses = [x['val_loss'] for x in outputs]\n",
    "    epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "    batch_accs = [x['val_acc'] for x in outputs]\n",
    "    epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "    return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    \n",
    "  \n",
    " def fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n",
    "    \n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(),lr)\n",
    "    for epoch in range(epochs):\n",
    "        start = datetime.now()\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in train_loader:\n",
    "            images, labels = batch \n",
    "            \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            out = model(images)                  # Generate predictions\n",
    "            loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "            #loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        #model.epoch_end(epoch, result)\n",
    "        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        print(f\"timer taken for epoch {epoch} is {datetime.now()-start}\")\n",
    "        history.append(result)\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaturalSceneClassification()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e6e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "num_epochs = 3\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001\n",
    "#fitting the model on training data and record the result after each epoch\n",
    "history = fit(num_epochs, lr, model, train_dl, val_dl, opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4324fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
