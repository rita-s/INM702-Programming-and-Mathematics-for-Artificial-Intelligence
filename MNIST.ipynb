{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INM702 Programming and Mathematics for Artificial Intelligence Coursework"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing required libraries: numpy for numerical operations, \n",
    "# matplotlib for plotting, TensorFlow's Keras for loading the MNIST dataset, \n",
    "# sklearn for confusion matrix, and seaborn for heatmap visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Loading and Preprocessing the MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Loading MNIST the dataset and preprocessing it (normalize pixel values, reshape, etc.)\n",
    "\n",
    "# Defining function\n",
    "def load_mnist_and_print():\n",
    "    \n",
    "    # Loading the MNIST dataset\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Splitting the dataset into training and testing sets, normalising and reshaping the data\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1) / 255.0\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
    "\n",
    "\n",
    "    # Printing a small part of the dataset\n",
    "    print(\"X_train sample shape:\", X_train.shape)\n",
    "    print(\"X_train sample:\", X_train[0])\n",
    "    print(\"y_train sample:\", y_train[0])\n",
    "    print(\"X_test sample shape:\", X_test.shape)\n",
    "    print(\"X_test sample:\", X_test[0])\n",
    "    print(\"y_test sample:\", y_test[0])\n",
    "\n",
    "# Calling the function\n",
    "load_mnist_and_print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() # Repeating code only because the output shows an error X_train not defined\n",
    "# Defining a function to plot a single image from the dataset:\n",
    "def plot_image(data, index):\n",
    "    plt.figure(figsize=(4, 4))  # Creating a figure with a size of 4x4 inches\n",
    "    image_2d = data[index].reshape(28, 28)  # Reshaping the flattened data back into 2D\n",
    "    plt.imshow(image_2d, cmap='gray')  # Displaying the image in grayscale\n",
    "    plt.title(f\"Label: {y_train[index]}\")  # Adding a title with the label of the image\n",
    "    plt.show()  # Displaying the plot\n",
    "\n",
    "# Visualising the image at a specific index\n",
    "plot_image(X_train, 0)  # This is index 0, we can see other images by changing the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(data, labels, start_index, num_images):\n",
    "    plt.figure(figsize=(8,8))  # Creating a figure with a specified size (8x8 in this case)\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(5, 5, i+1)  # Defining a 5x5 grid size and placing each image in sequence\n",
    "        plt.xticks([])  # Removing the x-axis tick marks\n",
    "        plt.yticks([])  # Removing the y-axis tick marks\n",
    "        # Reshape the image data from (784,) to (28, 28)\n",
    "        image_2d = data[start_index + i].reshape(28, 28)\n",
    "        plt.imshow(image_2d, cmap=plt.cm.binary)  # Displaying the image at the given index in grayscale\n",
    "        plt.xlabel(f\"Label: {labels[start_index + i]}\")  # Adding a label below the image showing its corresponding label from the dataset\n",
    "    plt.show()  # Displaying the entire set of subplots\n",
    "\n",
    "# Plotting the first 25 images from the dataset\n",
    "plot_images(X_train, y_train, 0, 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess the MNIST dataset (version 2):\n",
    "def load_and_preprocess_mnist_dataset():\n",
    "    # Loading the MNIST dataset from Keras\n",
    "    (training_images, training_labels), (testing_images, testing_labels) = mnist.load_data()\n",
    "\n",
    "    # Reshaping and normalising the images\n",
    "    # Changing the shape from (60000, 28, 28) to (60000, 784) for training images\n",
    "    # and from (10000, 28, 28) to (10000, 784) for testing images\n",
    "    # Normalising the pixel values to be between 0 and 1\n",
    "    training_images = training_images.reshape(60000, 784).astype('float32') / 255\n",
    "    testing_images = testing_images.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "    return training_images, training_labels, testing_images, testing_labels\n",
    "\n",
    "# Calling the function to load and preprocess the dataset\n",
    "training_images, training_labels, testing_images, testing_labels = load_and_preprocess_mnist_dataset()\n",
    "\n",
    "# Function to perform one-hot encoding of labels\n",
    "def one_hot_encode_labels(labels, number_of_classes):\n",
    "    # Using numpy's eye function to create a one-hot encoded matrix\n",
    "    return np.eye(number_of_classes)[labels]\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid_activation_function(input_array, calculating_derivative=False):\n",
    "    # The sigmoid function transforms input values to be between 0 and 1\n",
    "    if calculating_derivative:\n",
    "        # If calculating derivative, return the derivative of the sigmoid function\n",
    "        return input_array * (1 - input_array)\n",
    "    return 1 / (1 + np.exp(-input_array))\n",
    "\n",
    "# ReLU activation function\n",
    "def relu_activation_function(input_array, calculating_derivative=False):\n",
    "    # ReLU function sets all negative values in the input array to 0\n",
    "    if calculating_derivative:\n",
    "        # If calculating derivative, return 1 for all positive values and 0 for negative values\n",
    "        return np.where(input_array > 0, 1, 0)\n",
    "    return np.maximum(0, input_array)\n",
    "\n",
    "# Softmax function\n",
    "def softmax_function(input_array):\n",
    "    # The softmax function converts an array of numbers into a probability distribution\n",
    "    # Subtracting the max value for numerical stability\n",
    "    exponential_input_array = np.exp(input_array - np.max(input_array, axis=1, keepdims=True))\n",
    "    return exponential_input_array / np.sum(exponential_input_array, axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Dropout Layer\n",
    "class StandardDropoutLayer:\n",
    "    def __init__(self, dropout_rate):\n",
    "        # Initialising the dropout layer with a given dropout rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward_pass(self, input_data, training_mode):\n",
    "        # Applying dropout during the forward pass\n",
    "        if training_mode:\n",
    "            # Creating a mask with a binomial distribution\n",
    "            self.mask = np.random.binomial(1, 1 - self.dropout_rate, size=input_data.shape)\n",
    "            return input_data * self.mask\n",
    "        return input_data\n",
    "\n",
    "    def backward_pass(self, gradient_from_next_layer):\n",
    "        # During the backward pass, scaling the gradient by the saved mask\n",
    "        return gradient_from_next_layer * self.mask\n",
    "\n",
    "# Dense Layer with L2 Regularization\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size, l2_regularization_lambda=0.01):\n",
    "        # Initialising weights and biases for the dense layer\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.biases = np.zeros(output_size)\n",
    "        self.l2_regularization_lambda = l2_regularization_lambda\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        # Saving the input for use in the backward pass\n",
    "        self.input_data = input_data\n",
    "        # Calculating the output of the dense layer\n",
    "        self.output = np.dot(input_data, self.weights) + self.biases\n",
    "        return self.output\n",
    "\n",
    "    def backward_pass(self, gradient_from_next_layer):\n",
    "        # Calculating the gradient of the loss with respect to weights and biases\n",
    "        l2_regularization_term = self.l2_regularization_lambda * self.weights\n",
    "        self.weights_gradient = np.dot(self.input_data.T, gradient_from_next_layer) + l2_regularization_term\n",
    "        self.biases_gradient = np.sum(gradient_from_next_layer, axis=0)\n",
    "        # Calculating the gradient to be passed to the previous layer\n",
    "        gradient_to_previous_layer = np.dot(gradient_from_next_layer, self.weights.T)\n",
    "        return gradient_to_previous_layer\n",
    "\n",
    "# ReLU Activation Layer\n",
    "class ReLUActivationLayer:\n",
    "    def forward_pass(self, input_data):\n",
    "        # Applying ReLU activation (max(0, x))\n",
    "        self.input_data = input_data\n",
    "        return np.maximum(0, input_data)\n",
    "\n",
    "    def backward_pass(self, gradient_from_next_layer):\n",
    "        # Gradient of ReLU is 1 for positive input and 0 for negative input\n",
    "        gradient_to_previous_layer = gradient_from_next_layer * (self.input_data > 0)\n",
    "        return gradient_to_previous_layer\n",
    "\n",
    "# Sigmoid Activation Layer\n",
    "class SigmoidActivationLayer:\n",
    "    def forward_pass(self, input_data):\n",
    "        # Applying sigmoid activation\n",
    "        self.output = 1 / (1 + np.exp(-input_data))\n",
    "        return self.output\n",
    "\n",
    "    def backward_pass(self, gradient_from_next_layer):\n",
    "        # Gradient of sigmoid function\n",
    "        return gradient_from_next_layer * self.output * (1 - self.output)\n",
    "\n",
    "# Neural Network Class\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers_configuration, dropout_rate=0.5, regularizer=None):\n",
    "        # Initialising the neural network with a given configuration\n",
    "        self.layers = []\n",
    "        for layer_index, layer_configuration in enumerate(layers_configuration):\n",
    "            # Adding dense layers with specified input and output sizes\n",
    "            if 'dense' in layer_configuration:\n",
    "                input_size, output_size = layer_configuration['dense']\n",
    "                # Ensure the regularizer is a valid float\n",
    "                l2_lambda = regularizer if regularizer is not None else 0.01\n",
    "                self.layers.append(DenseLayer(input_size, output_size, l2_regularization_lambda=l2_lambda))\n",
    "            # Adding activation layers (ReLU or Sigmoid) based on configuration\n",
    "            elif 'activation' in layer_configuration:\n",
    "                if layer_configuration['activation'] == 'relu':\n",
    "                    self.layers.append(ReLUActivationLayer())\n",
    "                elif layer_configuration['activation'] == 'sigmoid':\n",
    "                    self.layers.append(SigmoidActivationLayer())\n",
    "            # Adding dropout layers except for the last layer\n",
    "            if layer_index < len(layers_configuration) - 1:\n",
    "                self.layers.append(StandardDropoutLayer(dropout_rate))\n",
    "\n",
    "\n",
    "    def forward_pass(self, input_data, training_mode=False):\n",
    "        # Forward pass through all layers\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, StandardDropoutLayer):\n",
    "                input_data = layer.forward_pass(input_data, training_mode)\n",
    "            else:\n",
    "                input_data = layer.forward_pass(input_data)\n",
    "        return input_data\n",
    "\n",
    "    def backward_pass(self, gradient_from_loss):\n",
    "        # Backward pass through all layers in reverse order\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient_from_loss = layer.backward_pass(gradient_from_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Entropy Loss Function\n",
    "def cross_entropy_loss_function(predictions, true_labels):\n",
    "    # Applying a small epsilon to avoid log(0)\n",
    "    epsilon = 1e-15\n",
    "    predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "\n",
    "    # Number of samples\n",
    "    number_of_samples = predictions.shape[0]\n",
    "\n",
    "    # Calculating the cross-entropy loss\n",
    "    cross_entropy_loss = -np.sum(true_labels * np.log(predictions)) / number_of_samples\n",
    "    return cross_entropy_loss\n",
    "\n",
    "# Gradient Computation for Loss\n",
    "def compute_gradient_with_respect_to_activation(predictions, true_labels):\n",
    "    number_of_samples = predictions.shape[0]\n",
    "    return (predictions - true_labels) / number_of_samples\n",
    "\n",
    "# Stochastic Gradient Descent Optimizer\n",
    "class StochasticGradientDescentOptimizer:\n",
    "    def __init__(self, learning_rate_value):\n",
    "        self.learning_rate_value = learning_rate_value\n",
    "\n",
    "    def update_parameters(self, layer):\n",
    "        # Updating weights and biases based on gradient and learning rate\n",
    "        layer.weights -= self.learning_rate_value * layer.weights_gradient\n",
    "        layer.biases -= self.learning_rate_value * layer.biases_gradient\n",
    "\n",
    "# Stochastic Gradient Descent with Momentum Optimizer\n",
    "class SGDWithMomentumOptimizer:\n",
    "    def __init__(self, learning_rate_value, momentum_value=0.9):\n",
    "        self.learning_rate_value = learning_rate_value\n",
    "        self.momentum_value = momentum_value\n",
    "\n",
    "    def update_parameters(self, layer):\n",
    "        # Initialising velocity for weights and biases if not already done\n",
    "        if not hasattr(layer, 'velocity_weights'):\n",
    "            layer.velocity_weights = np.zeros_like(layer.weights)\n",
    "            layer.velocity_biases = np.zeros_like(layer.biases)\n",
    "        # Updating velocities\n",
    "        layer.velocity_weights = self.momentum_value * layer.velocity_weights - self.learning_rate_value * layer.weights_gradient\n",
    "        layer.velocity_biases = self.momentum_value * layer.velocity_biases - self.learning_rate_value * layer.biases_gradient\n",
    "        # Updating weights and biases\n",
    "        layer.weights += layer.velocity_weights\n",
    "        layer.biases += layer.velocity_biases\n",
    "\n",
    "# Function to Train the Neural Network Model\n",
    "def train_neural_network_model(model, training_images, training_labels, validation_images, validation_labels, optimizer, total_epochs, batch_size_value, number_of_classes):\n",
    "    training_labels_encoded = one_hot_encode_labels(training_labels, number_of_classes)\n",
    "    validation_labels_encoded = one_hot_encode_labels(validation_labels, number_of_classes)\n",
    "    \n",
    "    training_history = {'training_loss': [], 'training_accuracy': [], 'validation_loss': [], 'validation_accuracy': []}\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "        # Training over mini-batches\n",
    "        for start_index in range(0, len(training_images), batch_size_value):\n",
    "            batch_training_images = training_images[start_index:start_index + batch_size_value]\n",
    "            batch_training_labels = training_labels_encoded[start_index:start_index + batch_size_value]\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model.forward_pass(batch_training_images, training_mode=True)\n",
    "\n",
    "            # Loss calculation\n",
    "            loss = cross_entropy_loss_function(predictions, batch_training_labels)\n",
    "\n",
    "            # Backward pass\n",
    "            gradient_from_loss = compute_gradient_with_respect_to_activation(predictions, batch_training_labels)\n",
    "            model.backward_pass(gradient_from_loss)\n",
    "\n",
    "            # Update parameters\n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, DenseLayer):\n",
    "                    optimizer.update_parameters(layer)\n",
    "\n",
    "        # Calculating training and validation metrics after each epoch\n",
    "        training_loss = cross_entropy_loss_function(model.forward_pass(training_images), training_labels_encoded)\n",
    "        training_accuracy = calculate_accuracy(model.forward_pass(training_images), training_labels)\n",
    "        validation_loss = cross_entropy_loss_function(model.forward_pass(validation_images), validation_labels_encoded)\n",
    "        validation_accuracy = calculate_accuracy(model.forward_pass(validation_images), validation_labels)\n",
    "\n",
    "        # Storing metrics in the history dictionary\n",
    "        training_history['training_loss'].append(training_loss)\n",
    "        training_history['training_accuracy'].append(training_accuracy)\n",
    "        training_history['validation_loss'].append(validation_loss)\n",
    "        training_history['validation_accuracy'].append(validation_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{total_epochs} - Training Loss: {training_loss:.4f}, Training Accuracy: {training_accuracy:.4f}, Validation Loss: {validation_loss:.4f}, Validation Accuracy: {validation_accuracy:.4f}\")\n",
    "\n",
    "    return training_history\n",
    "\n",
    "# Function to Calculate Accuracy\n",
    "def calculate_accuracy(predictions, true_labels):\n",
    "    # Argmax function converts the softmax probabilities to class labels\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "    # Comparing predicted labels to the true labels to find correct predictions\n",
    "    correct_predictions_count = np.sum(predicted_labels == true_labels)\n",
    "    # Calculating accuracy as the ratio of correct predictions to total predictions\n",
    "    accuracy_value = correct_predictions_count / len(true_labels)\n",
    "    return accuracy_value\n",
    "\n",
    "\n",
    "    # Calculating accuracy\n",
    "    accuracy = correct_predictions / true_labels.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "# Function to Plot Training History\n",
    "def plot_training_history(training_history):\n",
    "    # Plotting training and validation accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(training_history['training_accuracy'], label='Training Accuracy')\n",
    "    plt.plot(training_history['validation_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting training and validation loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(training_history['training_loss'], label='Training Loss')\n",
    "    plt.plot(training_history['validation_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Function to Plot Confusion Matrix\n",
    "def plot_confusion_matrix(model, images, labels, title='Confusion Matrix'):\n",
    "    # Predicting labels for the given images\n",
    "    predictions = np.argmax(model.forward_pass(images), axis=1)\n",
    "\n",
    "    # Generating a confusion matrix\n",
    "    confusion_matrix_values = confusion_matrix(labels, predictions)\n",
    "\n",
    "    # Plotting the confusion matrix as a heatmap\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(confusion_matrix_values, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage of the Extended Features\n",
    "# Splitting the training dataset into training and validation sets\n",
    "split_index = 50000\n",
    "validation_images, validation_labels = training_images[split_index:], training_labels[split_index:]\n",
    "training_images, training_labels = training_images[:split_index], training_labels[:split_index]\n",
    "\n",
    "# Initialising the model with a given configuration\n",
    "model_configuration = [\n",
    "    {'dense': (784, 128)}, {'activation': 'relu'},\n",
    "    {'dense': (128, 64)}, {'activation': 'relu'},\n",
    "    {'dense': (64, 10)}\n",
    "]\n",
    "\n",
    "# Creating the neural network model\n",
    "neural_network_model = NeuralNetwork(model_configuration, dropout_rate=0.5)\n",
    "\n",
    "# Training the model\n",
    "optimizer = SGDWithMomentumOptimizer(learning_rate_value=0.001, momentum_value=0.9)\n",
    "total_epochs = 1000\n",
    "batch_size_value = 128\n",
    "number_of_classes = 10\n",
    "\n",
    "training_history = train_neural_network_model(neural_network_model, training_images, training_labels, validation_images, validation_labels, optimizer, total_epochs, batch_size_value, number_of_classes)\n",
    "\n",
    "# Plotting the training history\n",
    "plot_training_history(training_history)\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plot_confusion_matrix(neural_network_model, validation_images, validation_labels)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
